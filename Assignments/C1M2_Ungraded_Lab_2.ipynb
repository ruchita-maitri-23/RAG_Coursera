{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4907a93a-28ba-4de7-8179-f88d2a6a3b48",
   "metadata": {},
   "source": [
    "# Ungraded Lab -  Retrieval Metrics\n",
    "---\n",
    "\n",
    "In this lab, you will be working on retrieving and analyzing metrics for a RAG system. RAG models are designed to improve the quality of generated responses by retrieving relevant documents from a knowledge base. Your goal is to evaluate the retrieval component by calculating precision and recall metrics, along with context precision and context recall.\n",
    "\n",
    "In this lab, you will learn:\n",
    "- How to compute precision and recall metrics\n",
    "- How to apply these metrics in information retrieval\n",
    "- How to work with a concrete dataset to test the retrieval capabilities of semantic-based searches\n",
    "\n",
    "You will be using the `sentence-transformers` library to convert text to embeddings, allowing efficient similarity computations. To compute retrieval metrics, you need a labeled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b358e3-bdf3-4764-a2c4-8a252b07226d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<h4 style=\"color:black; font-weight:bold;\">USING THE TABLE OF CONTENTS</h4>\n",
    "\n",
    "JupyterLab provides an easy way for you to navigate through your assignment. It's located under the Table of Contents tab, found in the left panel, as shown in the picture below.\n",
    "\n",
    "![TOC Location](images/toc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19728085",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - The dataset](#1)\n",
    "  - [ 1.1 Preprocessing and Vectorizing Data](#1-1)\n",
    "  - [ 1.2 Basic functions for retrieve](#1-2)\n",
    "- [ 2 - Retrieving metric](#2)\n",
    "  - [ 2.1 Precision](#2-1)\n",
    "  - [ 2.2 Recall](#2-2)\n",
    "  - [ 2.3 Computing metrics over some queries](#2-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e89a8d-a102-445a-b6e8-ec7f64146726",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 - Introduction\n",
    "---\n",
    "\n",
    "Retrieval metrics are fundamental in RAG systems, as they provide a way to measure performance. To effectively gauge performance, you need a labeled dataset—one where the answers to specific queries are known—allowing you to compare these results with those generated by your RAG system. In this lab, you will use a pre-labeled dataset and focus on Precision and Recall metrics.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/precision_recall.png\" alt=\"Description\" style=\"width: 70%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ac0621-1407-4486-8ec6-367b5c76de4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48feeb65-bd84-45fe-b8bf-a16fe6e34867",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1.1 The dataset\n",
    "\n",
    "The [20 Newsgroups dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) is a classic text dataset with text data on various topics, with labeled categories. Let's use the `sklearn.datasets` module to load this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35dacfa3-fed4-4268-b16b-db525e2727d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  category\n",
      "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...         7\n",
      "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...         4\n",
      "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...         4\n",
      "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...         1\n",
      "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...        14\n",
      "\n",
      "Dataset Size: (11314, 2)\n",
      "\n",
      "Number of Categories: 20\n",
      "\n",
      "Categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, data_home='./dataset')\n",
    "\n",
    "# Convert the dataset to a DataFrame for easier handling\n",
    "df = pd.DataFrame({\n",
    "    'text': newsgroups_train.data,\n",
    "    'category': newsgroups_train.target\n",
    "})\n",
    "\n",
    "# Display some basic information about the dataset\n",
    "print(df.head())\n",
    "print(\"\\nDataset Size:\", df.shape)\n",
    "print(\"\\nNumber of Categories:\", len(newsgroups_train.target_names))\n",
    "print(\"\\nCategories:\", newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfaa5af4-024c-4c06-964b-b16355eeee34",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:\n",
      "\tFrom: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CATEGORY:\n",
      "\trec.autos\n"
     ]
    }
   ],
   "source": [
    "print(f\"TEXT:\\n\\t{df['text'][0]}\\nCATEGORY:\\n\\t{newsgroups_train.target_names[df['category'][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd769e8-abd1-4607-bbc4-a729c5c9a3fd",
   "metadata": {},
   "source": [
    "<a id='1-1'></a>\n",
    "### 1.1 Preprocessing and Vectorizing Data\n",
    "\n",
    "In this section, you'll preprocess the text data by cleaning it and then vectorize the text using a pre-trained model from the `sentence-transformers` library. You will use the model `BAAI/bge-base-en-v1.5` for encoding the sentences into vectors. To save time, the dataset has been embedded ahead of time for you, so the model will be used only to vectorize the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618f87da-d13d-4d88-a931-3da5d1958c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained sentence transformer model\n",
    "model_name =  \"BAAI/bge-base-en-v1.5\"\n",
    "model = SentenceTransformer(os.path.join(os.environ['MODEL_PATH'],model_name))\n",
    "\n",
    "embedding_vectors = joblib.load('embeddings.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b44d25-39eb-4179-88fb-49d5fcd48135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f750f07-db2c-4ec7-a266-4bfc0ae8d77f",
   "metadata": {},
   "source": [
    "<a id='1-2'></a>\n",
    "### 1.2 Basic functions for retrieval\n",
    "\n",
    "Now let's implement a basic RAG mechanism by performing a similarity search over our precomputed embeddings. This code uses cosine similarity to find the most relevant documents for a given query. Let's first define our basic functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a237bea6-36ff-43c6-9532-6348350bf1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text data by removing leading and trailing whitespace.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    str: The preprocessed text, with leading and trailing whitespace removed.\n",
    "    \"\"\"\n",
    "    # Example preprocessing: remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def cosine_similarity(v1, array_of_vectors):\n",
    "    \"\"\"\n",
    "    Cosine similarity between a vector and either a single vector (1D) or an array of vectors (2D).\n",
    "    Returns a float for 1D input, or a list of floats for 2D input.\n",
    "    Safely handles PyTorch tensors (moves to CPU) and NumPy arrays.\n",
    "    \"\"\"\n",
    "    # Handle torch tensors for v1\n",
    "    if hasattr(v1, \"detach\"):  # torch tensor\n",
    "        v1 = v1.detach().cpu().numpy()\n",
    "    v1 = np.asarray(v1, dtype=np.float32).ravel()\n",
    "\n",
    "    # Handle torch tensors for array_of_vectors\n",
    "    if hasattr(array_of_vectors, \"detach\"):  # torch tensor\n",
    "        array_of_vectors = array_of_vectors.detach().cpu().numpy()\n",
    "    A = np.asarray(array_of_vectors, dtype=np.float32)\n",
    "\n",
    "    if A.ndim == 1:\n",
    "        A = A.ravel()\n",
    "        denom = np.linalg.norm(v1) * np.linalg.norm(A)\n",
    "        return float(0.0 if denom == 0 else np.dot(v1, A) / denom)\n",
    "\n",
    "    # 2D case: compute similarities for each row in A\n",
    "    A = np.atleast_2d(A)\n",
    "    v1_norm = np.linalg.norm(v1)\n",
    "    A_norms = np.linalg.norm(A, axis=1)\n",
    "    denom = v1_norm * A_norms\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        sims = (A @ v1) / np.where(denom == 0, 1.0, denom)\n",
    "    sims[denom == 0] = 0.0\n",
    "    return sims.tolist()\n",
    "\n",
    "\n",
    "def top_k_greatest_indices(lst, k):\n",
    "    \"\"\"\n",
    "    Get the indices of the top k greatest items in a list.\n",
    "\n",
    "    Parameters:\n",
    "    lst (list): The list of elements to evaluate.\n",
    "    k (int): The number of top elements to retrieve by index.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of indices corresponding to the top k greatest elements in lst.\n",
    "    \"\"\"\n",
    "    # Enumerate the list to keep track of indices\n",
    "    indexed_list = list(enumerate(lst))\n",
    "    # Sort by element values in descending order\n",
    "    sorted_by_value = sorted(indexed_list, key=lambda x: x[1], reverse=True)\n",
    "    # Extract the top k indices\n",
    "    top_k_indices = [index for index, value in sorted_by_value[:k]]\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87106c2d-e8de-4d89-933b-92fde4ed838d",
   "metadata": {},
   "source": [
    "Now let's define the retriever function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b546830a-c47b-47a5-b91f-07b4fbc7524d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: space exploration\n",
      "Document: From: u1452@penelope.sdsc.edu (Jeff Bytof - SIO)\n",
      "Subject: End of the Space Age?\n",
      "Organization: San Diego Supercomputer Center @ UCSD\n",
      "Lines: 16\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: penelope.sdsc.edu\n",
      "\n",
      "...\n",
      "Category: sci.space...\n",
      "\n",
      "\n",
      "\n",
      "Document: From: dennisn@ecs.comm.mot.com (Dennis Newkirk)\n",
      "Subject: Space class for teachers near Chicago\n",
      "Organization: Motorola\n",
      "Distribution: usa\n",
      "Nntp-Posting-Host: 145.1.146.43\n",
      "Lines: 59\n",
      "\n",
      "I am posting this for...\n",
      "Category: sci.space...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(query, embeddings, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar documents to a query using cosine similarity.\n",
    "    Assumes:\n",
    "      - preprocess_text, top_k_greatest_indices, df, and newsgroups_train are defined elsewhere.\n",
    "      - embeddings is an iterable of document embeddings (NumPy arrays or torch tensors).\n",
    "      - model.encode supports convert_to_tensor parameter (e.g., sentence-transformers).\n",
    "    \"\"\"\n",
    "    \n",
    "    query_clean = preprocess_text(query)\n",
    "    query_embedding = model.encode(query_clean, convert_to_tensor=False).astype(np.float32)\n",
    "\n",
    "    cosine_scores = []\n",
    "    for x in embeddings:\n",
    "        # Ensure each embedding is a NumPy array\n",
    "        if hasattr(x, \"detach\"):  # torch tensor\n",
    "            x = x.detach().cpu().numpy()\n",
    "        x = np.asarray(x, dtype=np.float32)\n",
    "\n",
    "        score = cosine_similarity(query_embedding, x)  # returns a float for 1D x\n",
    "        cosine_scores.append(float(score))\n",
    "\n",
    "    top_results = top_k_greatest_indices(cosine_scores, k=top_k)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    for idx in top_results:\n",
    "        print(f\"Document: {df.iloc[idx]['text'][:200]}...\")\n",
    "        print(f\"Category: {newsgroups_train.target_names[df.iloc[idx]['category']]}...\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "# Example query\n",
    "example_query = \"space exploration\"\n",
    "retrieve_documents(example_query, embedding_vectors, model, top_k = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0146f10-fd24-4cf4-8077-a0c9599b3248",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Retrieving metrics\n",
    "\n",
    "---\n",
    "\n",
    "Let's explore briefly the most common metrics for retrieval systems: Precision@K and Recall@K.\n",
    "\n",
    "<a id='2-1'></a>\n",
    "### 2.1 Precision@K\n",
    "\n",
    "Precision@K provides an evaluation of the relevancy of the top K retrieved documents. It's calculated as the ratio of relevant documents in the top K results to K (the total number of documents retrieved).\n",
    "\n",
    "$$\\text{Precision@K} = \\frac{\\text{Number of Relevant Documents in Top K}}{\\text{K}}$$\n",
    "\n",
    "where K is the number of documents retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d93434-8d66-46ff-85bc-5fd4f86a756e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precision_at_k(relevant_count, k):\n",
    "    \"\"\"\n",
    "    Calculate the Precision@K for a retrieval system.\n",
    "\n",
    "    Precision@K is the ratio of relevant documents in the top K retrieved documents\n",
    "    to the total number K of documents retrieved.\n",
    "\n",
    "    Args:\n",
    "        relevant_count (int): Number of relevant documents in the top K results.\n",
    "        k (int): Total number of documents retrieved (K).\n",
    "\n",
    "    Returns:\n",
    "        float: The Precision@K value, or 0.0 if k is zero.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any input is negative.\n",
    "    \"\"\"\n",
    "    if relevant_count < 0 or k < 0:\n",
    "        raise ValueError(\"All input values must be non-negative.\")\n",
    "    \n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return relevant_count / k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18b637-a290-45ca-8d00-15084a246a98",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Recall@K\n",
    "\n",
    "Recall@K evaluates the retrieval system's ability to find all relevant documents from the dataset within the top K results. It's calculated as the ratio of relevant documents in the top K results to the total number of relevant documents in the entire corpus.\n",
    "\n",
    "$$\\text{Recall@K} = \\frac{\\text{Number of Relevant Documents in Top K}}{\\text{Total Number of Relevant Documents in Corpus}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67291b40-1dd0-46f9-8f50-bad24a0de6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall_at_k(relevant_count, total_relevant):\n",
    "    \"\"\"\n",
    "    Calculate the Recall@K for a retrieval system.\n",
    "\n",
    "    Recall@K is the ratio of relevant documents in the top K retrieved documents\n",
    "    to the total number of relevant documents in the entire corpus.\n",
    "\n",
    "    Args:\n",
    "        relevant_count (int): Number of relevant documents in the top K results.\n",
    "        total_relevant (int): Total number of relevant documents in the corpus.\n",
    "\n",
    "    Returns:\n",
    "        float: The Recall@K value, or 0.0 if total_relevant is zero.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any input is negative.\n",
    "    \"\"\"\n",
    "    if relevant_count < 0 or total_relevant < 0:\n",
    "        raise ValueError(\"All input values must be non-negative.\")\n",
    "\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return relevant_count / total_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e07f2-d5e2-4abf-a5f6-4f115fe7be91",
   "metadata": {},
   "source": [
    "<a id='2-3'></a>\n",
    "### 2.3 Computing metrics over some queries\n",
    "\n",
    "Now let's compute these metrics on some pre-defined queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e204fdb-fbd2-4478-b3e7-a6055797104b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define more complex test queries with their corresponding desired categories\n",
    "test_queries = [\n",
    "    {\"query\": \"advancements in space exploration technology\", \"desired_category\": \"sci.space\"},\n",
    "    {\"query\": \"real-time rendering techniques in computer graphics\", \"desired_category\": \"comp.graphics\"},\n",
    "    {\"query\": \"latest findings in cardiovascular medical research\", \"desired_category\": \"sci.med\"},\n",
    "    {\"query\": \"NHL playoffs and team performance statistics\", \"desired_category\": \"rec.sport.hockey\"},\n",
    "    {\"query\": \"impacts of cryptography in online security\", \"desired_category\": \"sci.crypt\"},\n",
    "    {\"query\": \"the role of electronics in modern computing devices\", \"desired_category\": \"sci.electronics\"},\n",
    "    {\"query\": \"motorcycles maintenance tips for enthusiasts\", \"desired_category\": \"rec.motorcycles\"},\n",
    "    {\"query\": \"high-performance baseball tactics for championships\", \"desired_category\": \"rec.sport.baseball\"},\n",
    "    {\"query\": \"historical influence of politics on society\", \"desired_category\": \"talk.politics.misc\"},\n",
    "    {\"query\": \"latest technology trends in the Windows operating system\", \"desired_category\": \"comp.os.ms-windows.misc\"}\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab07ae82-e652-4eea-8bc4-55a9b481a782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(queries, embeddings, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Compute Precision@K and Recall@K for a list of queries against a dataset of document embeddings.\n",
    "    Assumes:\n",
    "      - preprocess_text, top_k_greatest_indices, precision_at_k, recall_at_k, df, newsgroups_train are defined elsewhere.\n",
    "      - embeddings is a list/iterable of document embeddings (NumPy arrays or torch tensors).\n",
    "      - model.encode supports convert_to_tensor parameter (e.g., sentence-transformers).\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Normalize all embeddings to NumPy once\n",
    "    np_embeddings = []\n",
    "    for x in embeddings:\n",
    "        if hasattr(x, \"detach\"):  # torch tensor\n",
    "            x = x.detach().cpu().numpy()\n",
    "        np_embeddings.append(np.asarray(x, dtype=np.float32).ravel())\n",
    "    E = np.vstack(np_embeddings)  # shape: (N, D)\n",
    "\n",
    "    for item in queries:\n",
    "        query = item[\"query\"]\n",
    "        desired_category = item[\"desired_category\"]\n",
    "\n",
    "        # Get NumPy, not torch, to avoid GPU->NumPy conversion errors\n",
    "        q_clean = preprocess_text(query)\n",
    "        q_emb = model.encode(q_clean, convert_to_tensor=False)\n",
    "        q_emb = np.asarray(q_emb, dtype=np.float32).ravel()\n",
    "\n",
    "        # Compute similarities vectorized\n",
    "        cosine_scores = cosine_similarity(q_emb, E)  # list of floats length N\n",
    "\n",
    "        # Top-K indices\n",
    "        top_results = top_k_greatest_indices(cosine_scores, k=top_k)\n",
    "\n",
    "        # Retrieved categories\n",
    "        retrieved_categories = [\n",
    "            newsgroups_train.target_names[df.iloc[idx][\"category\"]] for idx in top_results\n",
    "        ]\n",
    "\n",
    "        # Metrics\n",
    "        relevant_in_top_k = sum(1 for cat in retrieved_categories if cat == desired_category)\n",
    "        total_relevant_in_corpus = sum(\n",
    "            1 for idx in range(len(df))\n",
    "            if newsgroups_train.target_names[df.iloc[idx][\"category\"]] == desired_category\n",
    "        )\n",
    "\n",
    "        p = precision_at_k(relevant_in_top_k, top_k)\n",
    "        r = recall_at_k(relevant_in_top_k, total_relevant_in_corpus)\n",
    "\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"precision@k\": p,\n",
    "            \"recall@k\": r,\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72286c2d-e343-4cdb-8881-54b2e1be7c56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Results with K=5:\n",
      "================================================================================\n",
      "Query: advancements in space exploration technology\n",
      "  Precision@5: 1.00, Recall@5: 0.01\n",
      "\n",
      "Query: real-time rendering techniques in computer graphics\n",
      "  Precision@5: 1.00, Recall@5: 0.01\n",
      "\n",
      "Query: latest findings in cardiovascular medical research\n",
      "  Precision@5: 1.00, Recall@5: 0.01\n",
      "\n",
      "Query: NHL playoffs and team performance statistics\n",
      "  Precision@5: 1.00, Recall@5: 0.01\n",
      "\n",
      "Query: impacts of cryptography in online security\n",
      "  Precision@5: 1.00, Recall@5: 0.01\n",
      "\n",
      "Query: the role of electronics in modern computing devices\n",
      "  Precision@5: 1.00, Recall@5: 0.01\n",
      "\n",
      "Query: motorcycles maintenance tips for enthusiasts\n",
      "  Precision@5: 1.00, Recall@5: 0.01\n",
      "\n",
      "Query: high-performance baseball tactics for championships\n",
      "  Precision@5: 1.00, Recall@5: 0.01\n",
      "\n",
      "Query: historical influence of politics on society\n",
      "  Precision@5: 0.40, Recall@5: 0.00\n",
      "\n",
      "Query: latest technology trends in the Windows operating system\n",
      "  Precision@5: 0.80, Recall@5: 0.01\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Results with K=20:\n",
      "================================================================================\n",
      "Query: advancements in space exploration technology\n",
      "  Precision@20: 1.00, Recall@20: 0.03\n",
      "\n",
      "Query: real-time rendering techniques in computer graphics\n",
      "  Precision@20: 1.00, Recall@20: 0.03\n",
      "\n",
      "Query: latest findings in cardiovascular medical research\n",
      "  Precision@20: 1.00, Recall@20: 0.03\n",
      "\n",
      "Query: NHL playoffs and team performance statistics\n",
      "  Precision@20: 1.00, Recall@20: 0.03\n",
      "\n",
      "Query: impacts of cryptography in online security\n",
      "  Precision@20: 1.00, Recall@20: 0.03\n",
      "\n",
      "Query: the role of electronics in modern computing devices\n",
      "  Precision@20: 0.80, Recall@20: 0.03\n",
      "\n",
      "Query: motorcycles maintenance tips for enthusiasts\n",
      "  Precision@20: 0.95, Recall@20: 0.03\n",
      "\n",
      "Query: high-performance baseball tactics for championships\n",
      "  Precision@20: 1.00, Recall@20: 0.03\n",
      "\n",
      "Query: historical influence of politics on society\n",
      "  Precision@20: 0.50, Recall@20: 0.02\n",
      "\n",
      "Query: latest technology trends in the Windows operating system\n",
      "  Precision@20: 0.65, Recall@20: 0.02\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Results with K=50:\n",
      "================================================================================\n",
      "Query: advancements in space exploration technology\n",
      "  Precision@50: 1.00, Recall@50: 0.08\n",
      "\n",
      "Query: real-time rendering techniques in computer graphics\n",
      "  Precision@50: 0.88, Recall@50: 0.08\n",
      "\n",
      "Query: latest findings in cardiovascular medical research\n",
      "  Precision@50: 1.00, Recall@50: 0.08\n",
      "\n",
      "Query: NHL playoffs and team performance statistics\n",
      "  Precision@50: 0.98, Recall@50: 0.08\n",
      "\n",
      "Query: impacts of cryptography in online security\n",
      "  Precision@50: 1.00, Recall@50: 0.08\n",
      "\n",
      "Query: the role of electronics in modern computing devices\n",
      "  Precision@50: 0.66, Recall@50: 0.06\n",
      "\n",
      "Query: motorcycles maintenance tips for enthusiasts\n",
      "  Precision@50: 0.98, Recall@50: 0.08\n",
      "\n",
      "Query: high-performance baseball tactics for championships\n",
      "  Precision@50: 1.00, Recall@50: 0.08\n",
      "\n",
      "Query: historical influence of politics on society\n",
      "  Precision@50: 0.52, Recall@50: 0.06\n",
      "\n",
      "Query: latest technology trends in the Windows operating system\n",
      "  Precision@50: 0.60, Recall@50: 0.05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the queries and compute metrics with different K values\n",
    "k_values = [5, 20, 50]\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Results with K={k}:\")\n",
    "    print('='*80)\n",
    "    results = compute_metrics(test_queries, embedding_vectors, model, top_k=k)\n",
    "    \n",
    "    # Display the results\n",
    "    for result in results:\n",
    "        print(f\"Query: {result['query']}\")\n",
    "        print(f\"  Precision@{k}: {result['precision@k']:.2f}, Recall@{k}: {result['recall@k']:.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45998ce3-e314-4a4a-ac54-97f57812f7a8",
   "metadata": {},
   "source": [
    "**Understanding the Results:**\n",
    "\n",
    "The results above clearly demonstrate the **precision-recall tradeoff** in retrieval systems as we vary K from 5 to 20 to 50:\n",
    "\n",
    "**Precision@K Trends (generally decreases as K increases):**\n",
    "\n",
    "- **At K=5**: Most queries achieve very high precision (0.80-1.00), with 8 out of 10 queries having perfect precision (1.00). This means nearly all retrieved documents are highly relevant.\n",
    "  \n",
    "- **At K=20**: Precision starts to decline for some queries:\n",
    "  - \"electronics in computing devices\" drops to 0.80 (from 1.00)\n",
    "  - \"Windows operating system\" drops to 0.65 (from 0.80)\n",
    "  - \"motorcycles maintenance\" drops to 0.95 (from 1.00)\n",
    "  \n",
    "- **At K=50**: Precision decreases further as we retrieve more documents:\n",
    "  - \"computer graphics\" drops to 0.88 (from 1.00)\n",
    "  - \"electronics in computing devices\" drops to 0.66 (from 0.80)\n",
    "  - \"Windows operating system\" drops to 0.60 (from 0.65)\n",
    "  - \"historical influence of politics\" remains around 0.50-0.52 (the lowest across all K values)\n",
    "\n",
    "**Recall@K Trends (increases as K increases):**\n",
    "\n",
    "- **At K=5**: Recall is very low (~0.01 or 1%), meaning only about 1% of all relevant documents are retrieved\n",
    "  \n",
    "- **At K=20**: Recall triples to ~0.03 (3%), capturing more relevant documents\n",
    "  \n",
    "- **At K=50**: Recall increases to 0.05-0.08 (5-8%), capturing approximately 8 times more relevant documents than K=5\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **The tradeoff is clear**: As K increases, we retrieve more of the total relevant documents (higher recall), but at the cost of including some irrelevant documents (lower precision).\n",
    "\n",
    "2. **Some queries are harder than others**: The query \"historical influence of politics on society\" consistently shows the lowest precision (0.40-0.52), suggesting that this query is semantically ambiguous or the category \"talk.politics.misc\" is harder to distinguish from related categories.\n",
    "\n",
    "3. **For RAG systems, K=5 to K=20 is often optimal**: These values provide high precision (most retrieved documents are relevant) while keeping the context size manageable for the LLM. Even though recall is low, the goal is to find the *most relevant* documents, not *all* relevant documents.\n",
    "\n",
    "4. **Recall remains relatively low even at K=50**: This is expected since each category contains hundreds of documents (500-600), so retrieving 50 documents only captures ~8-10% of the total relevant documents. To achieve high recall, we would need K values in the hundreds, which would severely impact precision and be impractical for RAG applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc08ef-4c46-4bb5-84c6-272fb6ab5923",
   "metadata": {},
   "source": [
    "Congratulations on finishing this Ungraded Lab! Keep it up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
